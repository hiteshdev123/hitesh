{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":761188,"sourceType":"datasetVersion","datasetId":395773},{"sourceId":1315164,"sourceType":"datasetVersion","datasetId":762074},{"sourceId":2410564,"sourceType":"datasetVersion","datasetId":1458207},{"sourceId":3559828,"sourceType":"datasetVersion","datasetId":2139628},{"sourceId":6167163,"sourceType":"datasetVersion","datasetId":3532002}],"dockerImageVersionId":30558,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q alibi-detect","metadata":{"execution":{"iopub.status.busy":"2023-10-12T12:50:40.464736Z","iopub.execute_input":"2023-10-12T12:50:40.465349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nprint(tf.__version__)\n\nfrom tensorflow.keras.layers import Conv2D, Conv2DTranspose, Dense, Reshape, InputLayer, Flatten\n\nfrom alibi_detect.od import OutlierAE, OutlierVAE\nfrom alibi_detect.utils.visualize import plot_instance_score, plot_feature_outlier_image\n\nimport logging\nfrom tensorflow.keras.layers import Conv2D, Conv2DTranspose, \\\n    Dense, Layer, Reshape, InputLayer, Flatten\nfrom tqdm import tqdm\n\nfrom alibi_detect.od import OutlierAE\nfrom alibi_detect.utils.fetching import fetch_detector\nfrom alibi_detect.utils.perturbation import apply_mask\nfrom alibi_detect.saving import save_detector, load_detector\nfrom alibi_detect.utils.visualize import plot_instance_score, plot_feature_outlier_image\n\nlogger = tf.get_logger()\nlogger.setLevel(logging.ERROR)","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.431346Z","iopub.execute_input":"2023-10-08T23:47:11.431797Z","iopub.status.idle":"2023-10-08T23:47:11.497742Z","shell.execute_reply.started":"2023-10-08T23:47:11.431763Z","shell.execute_reply":"2023-10-08T23:47:11.491862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2 as cv\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport pandas as pd \nimport random\nimport tensorflow as tf\nfrom PIL import Image\nfrom tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras import layers, models\nfrom alibi_detect.od import OutlierAE\ntf.keras.backend.clear_session()\n\n# Set a seed value\nseed_value = 42\n\nimport os\nos.environ['PYTHONHASHSEED'] = str(seed_value)\nrandom.seed(seed_value)\nnp.random.seed(seed_value)\ntf.random.set_seed(seed_value)","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.499038Z","iopub.status.idle":"2023-10-08T23:47:11.50038Z","shell.execute_reply.started":"2023-10-08T23:47:11.500065Z","shell.execute_reply":"2023-10-08T23:47:11.500094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Several libraries are imported, including:\n    - os: For interacting with the operating system.\n    - cv2: OpenCV library for computer vision tasks.\n    - PIL: Python Imaging Library for image processing tasks.\n    - numpy: For numerical computations.\n    - matplotlib.pyplot: For plotting visualizations.\n    - tensorflow: For building and training machine learning models.","metadata":{}},{"cell_type":"code","source":"def plot5(X):\n    o = 0\n    for i in X:\n        o += 1\n        if o >= 5:\n            break\n        plt.figure(figsize=(2,2))  # Corrected 'figuresize' to 'figsize'\n        plt.imshow(i)\n        plt.show()\n\n        \ndef plot_5_random_images_per_class(X, df):\n    # Combine images and labels\n    combined_data = list(zip(X, df.values))\n\n    # Shuffle the combined data\n    np.random.shuffle(combined_data,)\n\n    # Create a figure with 5 subplots\n    fig, axes = plt.subplots(nrows=4, ncols=5, figsize=(10, 7))\n\n    # Initialize counters for each class\n    counters = [0, 0, 0, 0]\n\n    # Iterate over the shuffled images and labels\n    for img, labels in combined_data:\n        # Iterate over labels and their corresponding counters\n        for label, count in enumerate(counters):\n            if labels[label] == 1 and count < 5:\n                # Plot images with the corresponding label\n                axes[label][count].imshow(img)\n                axes[label][count].axis('off')\n                axes[label][count].set_title(f'Label {label}')\n                counters[label] += 1\n\n        # Check if we've plotted enough images for each class\n        if all(count == 5 for count in counters):\n            break\n\n    # Show the figure\n    plt.tight_layout()\n    plt.show()\n    \ndef plot_loss(history, num_epochs):\n    if num_epochs > len(history.history['loss']):\n        raise ValueError(\"num_epochs should be less than or equal to the total number of epochs in the history.\")\n\n    start_epoch = len(history.history['loss']) - num_epochs\n    end_epoch = len(history.history['loss'])\n\n    plt.figure(figsize=(6, 4))\n    plt.plot(range(start_epoch + 1, end_epoch + 1), history.history['loss'][start_epoch:], label='Training Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title(f'Loss for Epochs {start_epoch + 1} - {end_epoch}')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\ndef plot_training_history(history, from_=0):\n    # Get training and validation loss from history\n    training_loss = history.history['loss']\n    validation_loss = history.history['val_loss']\n    l = len(training_loss)\n    if from_:\n        training_loss = training_loss[-from_:]\n        validation_loss = validation_loss[-from_:]\n\n    # Number of epochs\n\n    epochs = range(l-from_, l)\n\n    # Plotting the training and validation loss\n    plt.figure(figsize=(6, 4))\n    plt.plot(epochs, training_loss, 'b', label='Training Loss')\n    plt.plot(epochs, validation_loss, 'r', label='Validation Loss')\n    plt.title('Training and Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.502002Z","iopub.status.idle":"2023-10-08T23:47:11.502834Z","shell.execute_reply.started":"2023-10-08T23:47:11.50254Z","shell.execute_reply":"2023-10-08T23:47:11.502567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Functions are defined:\n    - `plot5(X)`: Plots the first four images from a given list X.\n    - `plot_5_random_images_per_class(X, df)`: Plots five random images per class from a combined dataset of images and their labels.\n    - `plot_loss(history, num_epochs)`: Plots the training loss for a specified range of epochs.\n    - `plot_training_history(history, from_=0)`: Plots the training and validation loss over epochs, optionally starting from a specified epoch.","metadata":{}},{"cell_type":"markdown","source":"# Loading datasets\nThe code sets up the data for training a drowsiness detection model. It combines images from various sources, prepares labels, splits the data, and preprocesses it for TensorFlow datasets.\n\n**Loading Images from Directories:**\n- Several paths to directories containing image data are specified. Images are loaded from these directories using the `load_images` function.","metadata":{}},{"cell_type":"code","source":"def load_images(yawn_path, no_yawn_path, target_shape=(32,32)):\n    yawn_images = []\n    no_yawn_images = []\n    \n    for filename in os.listdir(yawn_path):\n        if filename.endswith(\".jpg\") or filename.endswith(\".png\"): \n            image_path = os.path.join(yawn_path, filename)\n            img = Image.open(image_path)\n            img = img.convert(\"RGB\")  # Convert to RGB mode\n            if target_shape is not None:\n                img = img.resize(target_shape)  # Resize image\n            yawn_images.append(np.array(img))\n    \n    for filename in os.listdir(no_yawn_path):\n        if filename.endswith(\".jpg\") or filename.endswith(\".png\"): \n            image_path = os.path.join(no_yawn_path, filename)\n            img = Image.open(image_path)\n            img = img.convert(\"RGB\")  # Convert to RGB mode\n            if target_shape is not None:\n                img = img.resize(target_shape)  # Resize image\n            no_yawn_images.append(np.array(img))\n    \n    return np.array(yawn_images), np.array(no_yawn_images)","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.504393Z","iopub.status.idle":"2023-10-08T23:47:11.505322Z","shell.execute_reply.started":"2023-10-08T23:47:11.504982Z","shell.execute_reply":"2023-10-08T23:47:11.505009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yawn_path = '/kaggle/input/detectyawning/train/0'\nno_yawn_path = '/kaggle/input/detectyawning/train/1'\n\nyawn_images, no_yawn_images = load_images(yawn_path, no_yawn_path)\nyawn_images.shape, no_yawn_images.shape","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.506603Z","iopub.status.idle":"2023-10-08T23:47:11.507728Z","shell.execute_reply.started":"2023-10-08T23:47:11.507439Z","shell.execute_reply":"2023-10-08T23:47:11.507466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_yawn2_path = '/kaggle/input/yawn-dataset/no yawn'\nyawn2_path = '/kaggle/input/yawn-dataset/yawn'\n\nyawn2_images, no_yawn2_images = load_images(yawn2_path, no_yawn2_path)\nyawn2_images.shape, no_yawn2_images.shape","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.509136Z","iopub.status.idle":"2023-10-08T23:47:11.509904Z","shell.execute_reply.started":"2023-10-08T23:47:11.509627Z","shell.execute_reply":"2023-10-08T23:47:11.509672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"closed_path = '/kaggle/input/openned-closed-eyes/TrainingSet/TrainingSet/Closed'\nopen_path = '/kaggle/input/openned-closed-eyes/TrainingSet/TrainingSet/Opened'\n\ntest_closed_path = '/kaggle/input/openned-closed-eyes/TestSet/TestSet/Closed'\ntest_open_path = '/kaggle/input/openned-closed-eyes/TestSet/TestSet/Opened'\n\nclosed_images, open_images = load_images(closed_path, open_path)\nprint(closed_images.shape, open_images.shape)\n\ntest_closed_images, test_open_images = load_images(test_closed_path, test_open_path)\nprint(test_closed_images.shape, test_open_images.shape)","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.511104Z","iopub.status.idle":"2023-10-08T23:47:11.512367Z","shell.execute_reply.started":"2023-10-08T23:47:11.512082Z","shell.execute_reply":"2023-10-08T23:47:11.512114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"closed_path = '/kaggle/input/yawn-eye-dataset-new/dataset_new/train/Closed'\nopen_path = '/kaggle/input/yawn-eye-dataset-new/dataset_new/train/Open'\ntest_closed_path = '/kaggle/input/yawn-eye-dataset-new/dataset_new/test/Closed'\ntest_open_path = '/kaggle/input/yawn-eye-dataset-new/dataset_new/test/Open'\n\nyawn_path = '/kaggle/input/yawn-eye-dataset-new/dataset_new/train/yawn'\nno_yawn_path = '/kaggle/input/yawn-eye-dataset-new/dataset_new/train/no_yawn'\ntest_yawn_path = '/kaggle/input/yawn-eye-dataset-new/dataset_new/test/yawn'\ntest_no_yawn_path = '/kaggle/input/yawn-eye-dataset-new/dataset_new/test/no_yawn'\n\nprint('Eyes')\nclosed_images3, open_images3 = load_images(closed_path, open_path)\nprint(closed_images3.shape, open_images3.shape)\n\ntest_closed_images3, test_open_images3 = load_images(test_closed_path, test_open_path)\nprint(test_closed_images3.shape, test_open_images3.shape)\n\nprint('\\nYawn')\nyawn_images3, no_yawn_images3 = load_images(yawn_path, no_yawn_path)\nprint(yawn_images3.shape, no_yawn_images3.shape)\ntest_yawn_images3, test_no_yawn_images3 = load_images(test_yawn_path, test_no_yawn_path)\nprint(test_yawn_images3.shape, test_no_yawn_images3.shape)","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.513874Z","iopub.status.idle":"2023-10-08T23:47:11.515078Z","shell.execute_reply.started":"2023-10-08T23:47:11.514827Z","shell.execute_reply":"2023-10-08T23:47:11.514854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The shapes of the loaded image arrays are printed. This provides information on the dimensions of the image datasets.","metadata":{}},{"cell_type":"markdown","source":"## Combine all eyes\n\n**Combining Images:**\n\n- Images from different datasets (yawn, no yawn, closed eyes, open eyes) are stacked vertically using `np.vstack()` to create combined datasets for each category (yawn, no yawn, closed eyes, open eyes).","metadata":{}},{"cell_type":"code","source":"# Combine images and labels\nall_yawn = np.vstack((yawn_images,\n                      yawn2_images,\n                      yawn_images3,\n                      test_yawn_images3))\n\nall_no_yawn = np.vstack((no_yawn_images,\n                        no_yawn2_images,\n                        no_yawn_images3,\n                        test_no_yawn_images3))\n\nall_close = np.vstack((closed_images,\n                        test_closed_images,\n                        closed_images3,\n                        test_closed_images3))\n\nall_open = np.vstack((open_images,\n                        test_open_images,\n                        open_images3,\n                        test_open_images3))\n\nall_yawn_y = np.full(len(all_yawn),'yawn')\nall_no_yawn_y = np.full(len(all_no_yawn),'no_yawn')\nall_close_y = np.full(len(all_close),'close')\nall_open_y = np.full(len(all_open),'open')","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.516379Z","iopub.status.idle":"2023-10-08T23:47:11.516812Z","shell.execute_reply.started":"2023-10-08T23:47:11.516628Z","shell.execute_reply":"2023-10-08T23:47:11.516661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Creating Labels:**\n\n- Corresponding labels are generated for each combined dataset using `np.full()`. These labels indicate the category of each image.","metadata":{}},{"cell_type":"code","source":"# all_labels = np.hstack((closed_labels, open_labels, test_closed_labels, test_open_labels))\nprint('Yawn')\nprint(all_yawn.shape,all_yawn_y.shape,)\nprint('No Yawn')\nprint(all_no_yawn.shape,all_no_yawn_y.shape,)\nprint('Close')\nprint(all_close.shape,all_close_y.shape,)\nprint('Open')\nprint(all_open.shape,all_open_y.shape,)","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.518626Z","iopub.status.idle":"2023-10-08T23:47:11.519055Z","shell.execute_reply.started":"2023-10-08T23:47:11.518879Z","shell.execute_reply":"2023-10-08T23:47:11.518896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Combining Labels:**\n- The labels for all categories are concatenated into a single array using `np.concatenate()`.\n\n**One-Hot Encoding:**\n- The labels are converted into a one-hot encoded DataFrame using pd.get_dummies(). This creates binary columns for each category.\n\nOne-hot encoding is a technique used to represent categorical data in a format suitable for AI models. In this context, the labels, which indicate the category of each image (such as 'yawn' or 'no yawn'), are transformed into a binary matrix. Each category becomes a column, with a '1' indicating the presence of that category and '0' indicating its absence. This representation enables the model to interpret categorical information effectively, improving its ability to make accurate predictions based on the labels. The `pd.get_dummies()` function from the pandas library automates this transformation, creating a structured DataFrame for easy integration into the training process.\n\n**Setting DataFrame Index (Optional):**\n- An index for the DataFrame is set using df.index if needed.\n\n**Combining Image Data:**\n- The image data is concatenated into a single array X using np.concatenate().","metadata":{}},{"cell_type":"code","source":"# Combine labels\nlabels = np.concatenate([all_yawn_y, all_no_yawn_y, all_close_y, all_open_y])\n\n# Create one-hot encoded DataFrame\ndf = pd.get_dummies(labels, columns=['Labels'],dtype=int)\n\n# Optionally, you can set an appropriate index for the DataFrame if needed\ndf.index = np.arange(len(df))\n\n# X is your training data\nX = np.concatenate([all_yawn, all_no_yawn, all_close, all_open])\n\n# Display the resulting DataFrame (labels for images)","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.52021Z","iopub.status.idle":"2023-10-08T23:47:11.520557Z","shell.execute_reply.started":"2023-10-08T23:47:11.520395Z","shell.execute_reply":"2023-10-08T23:47:11.52041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X[0][5][0] # view value of single pixel","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.521598Z","iopub.status.idle":"2023-10-08T23:47:11.521971Z","shell.execute_reply.started":"2023-10-08T23:47:11.521804Z","shell.execute_reply":"2023-10-08T23:47:11.521819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Normalizing Image Data:**\n\nThe image data is normalized by dividing by 255 to bring pixel values within the range [0, 1].\n\nIt is a crucial step in data preprocessing for AI models. By dividing the pixel values by 255, we scale them down to a range between 0 and 1. This standardizes the data, making it easier for the model to learn and converge during training. Additionally, it helps prevent numerical stability issues that can arise when working with large pixel values. This normalization process ensures that each feature (pixel) contributes equally to the model's learning process, leading to more effective training and accurate predictions.","metadata":{}},{"cell_type":"code","source":"X = X.astype('float32') / 255","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.525874Z","iopub.status.idle":"2023-10-08T23:47:11.526244Z","shell.execute_reply.started":"2023-10-08T23:47:11.526085Z","shell.execute_reply":"2023-10-08T23:47:11.526101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X[0][5][0]  # view value of single pixel after scaling","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.5277Z","iopub.status.idle":"2023-10-08T23:47:11.528444Z","shell.execute_reply.started":"2023-10-08T23:47:11.528274Z","shell.execute_reply":"2023-10-08T23:47:11.528293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.529596Z","iopub.status.idle":"2023-10-08T23:47:11.530006Z","shell.execute_reply.started":"2023-10-08T23:47:11.529823Z","shell.execute_reply":"2023-10-08T23:47:11.52984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# np.savez_compressed('images_data.npz', my_array=X)\n# file_size_bytes = os.path.getsize('/kaggle/working/images_data.npz')\n# file_size_kb = file_size_bytes / 1024\n# file_size_mb = file_size_kb / 1024\n# file_size_gb = file_size_mb / 1024\n# file_size_mb","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.531723Z","iopub.status.idle":"2023-10-08T23:47:11.533072Z","shell.execute_reply.started":"2023-10-08T23:47:11.532877Z","shell.execute_reply":"2023-10-08T23:47:11.532898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df.to_csv('labels.csv')","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.534167Z","iopub.status.idle":"2023-10-08T23:47:11.534541Z","shell.execute_reply.started":"2023-10-08T23:47:11.534353Z","shell.execute_reply":"2023-10-08T23:47:11.534376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_5_random_images_per_class(X, df)","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.536175Z","iopub.status.idle":"2023-10-08T23:47:11.536734Z","shell.execute_reply.started":"2023-10-08T23:47:11.536529Z","shell.execute_reply":"2023-10-08T23:47:11.536546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split data\n\n`train_test_split()` function, a common tool in machine learning. The split is configured to allocate 80% of the data for training, 10% for validation, and another 10% for testing. This allocation ensures that the model has ample data to learn from during training, a separate set for tuning hyperparameters (validation), and a final independent set to assess its performance accurately. This process helps prevent overfitting and provides a reliable evaluation of the model's generalization capabilities.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nprint(X.shape)","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.537583Z","iopub.status.idle":"2023-10-08T23:47:11.538896Z","shell.execute_reply.started":"2023-10-08T23:47:11.538683Z","shell.execute_reply":"2023-10-08T23:47:11.53871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split data into train, validation, and test sets (80% train, 10% validation, 10% test)\nX_train, X_temp, y_train, y_temp = train_test_split(X, df.values, test_size=0.2, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\nprint(X_train.shape, y_train.shape)\nprint(X_val.shape, y_val.shape)\nprint(X_test.shape, y_test.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.53973Z","iopub.status.idle":"2023-10-08T23:47:11.54077Z","shell.execute_reply.started":"2023-10-08T23:47:11.540544Z","shell.execute_reply":"2023-10-08T23:47:11.540565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# close no_yawn open yawn\n# printing count of images for each category in each data split\nprint('Train:',np.sum(y_train, axis=0), len(y_train))\nprint('Valid:',np.sum(y_val, axis=0), len(y_val))\nprint('Test :',np.sum(y_test, axis=0), len(y_test))","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.541496Z","iopub.status.idle":"2023-10-08T23:47:11.541853Z","shell.execute_reply.started":"2023-10-08T23:47:11.541678Z","shell.execute_reply":"2023-10-08T23:47:11.541703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Creating TensorFlow Datasets:**\n- TensorFlow datasets are created from the numpy arrays for training, validation, and testing.\n\nNumpy arrays containing the image and label data are converted into TensorFlow datasets. These datasets are specialized data structures that allow for efficient handling of large amounts of data during training. This conversion ensures that the data can be seamlessly fed into a neural network model built with TensorFlow, enabling efficient training and evaluation processes.\n\n**Preprocessing Images:**\n- A preprocessing function `preprocess_image()` is defined to resize images to (32, 32) and normalize them to the range [0, 1].\n- The preprocessing function is applied to the datasets using the map() function.\n\n\n**Shuffling and Batching:**\n- Datasets are shuffled and batched to prepare them for training.\n\nShuffling randomizes the order of the data samples, preventing the model from learning patterns based on the sequence of input. Batching involves grouping a specified number of samples together. This allows the model to process a batch of data at a time, which can significantly speed up training, especially on parallel computing platforms like GPUs. These steps collectively help ensure that the model learns from a diverse and varied set of examples, leading to more robust and accurate predictions.","metadata":{}},{"cell_type":"code","source":"AUTOTUNE = tf.data.AUTOTUNE\n\n# Create TensorFlow Datasets\ntrain_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\nval_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\ntest_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n\n# Define preprocessing function (resize and normalize)\ndef preprocess_image(image, label):\n    image = tf.image.resize(image, (32, 32))\n    image = image / 255.0  # Normalize to [0,1]\n    return image, label\n\n# Apply preprocessing to the datasets\ntrain_dataset = train_dataset.map(preprocess_image)\nval_dataset = val_dataset.map(preprocess_image)\ntest_dataset = test_dataset.map(preprocess_image)\n\n# Shuffle and batch the datasets\nBATCH_SIZE = 32\nSHUFFLE_BUFFER_SIZE = 1000\n\ntrain_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\nval_dataset = val_dataset.batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\ntest_dataset = test_dataset.batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.543427Z","iopub.status.idle":"2023-10-08T23:47:11.543775Z","shell.execute_reply.started":"2023-10-08T23:47:11.543595Z","shell.execute_reply":"2023-10-08T23:47:11.54361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model creation\n\n**Setting Random Seeds & Clearing Session and Resetting State:**\n- The random seeds for NumPy and TensorFlow are set to ensure reproducibility of results.\n- The TensorFlow session and states generated by Keras are cleared to start with a clean slate.\n\n**Defining the Model:**\n- A sequential model is created using Keras. It consists of multiple layers:\n    - Three convolutional layers with different filter sizes, activation functions, and batch normalization. Each layer is followed by max pooling and dropout.\n    - Flattening layer to prepare for fully connected layers.\n    - Two dense (fully connected) layers with ReLU activation functions.\n    - A final dense layer with a softmax activation function, outputting probabilities for each class.\n\nThis model architecture is well-suited for your drowsiness detection project due to its design tailored for image classification tasks. The initial convolutional layers apply various filters to extract relevant features from the input images, effectively capturing spatial patterns. The addition of batch normalization helps stabilize the learning process. Subsequent max pooling reduces spatial dimensions, preserving essential information while reducing computational complexity. Dropout layers further enhance model generalization by mitigating overfitting. The flattening layer restructures the data for fully connected layers, allowing the model to learn complex relationships between features. The dense layers with ReLU activations enable the model to capture higher-level abstractions. The final dense layer with softmax activation produces class probabilities, making it suitable for multi-class classification. This architecture leverages the strengths of convolutional neural networks for image analysis, making it apt for identifying drowsiness from visual data.\n\n**Compiling the Model:**\n- The model is compiled with an `'adam' optimizer, categorical cross-entropy loss` (since labels are one-hot encoded), and accuracy as the evaluation metric.\n\n`'Adam'` combines the advantages of both RMSprop and Momentum, adapting learning rates for individual parameters. This adaptability is particularly beneficial for complex, high-dimensional problems, enhancing the model's ability to efficiently converge during training. Moreover, employing categorical `cross-entropy` as the loss function is well-suited for multi-class classification tasks with one-hot encoded labels, effectively penalizing deviations from the true classes. Finally, evaluating model performance using accuracy as a metric aligns perfectly with the objective of correctly classifying drowsiness levels. This combination of optimizer, loss function, and evaluation metric is strategically chosen to optimize the model's learning process and enhance its performance in your specific project.\n","metadata":{}},{"cell_type":"code","source":"# Set random seed\nnp.random.seed(42)\ntf.random.set_seed(42)\ntf.keras.backend.clear_session()\ntf.keras.utils.set_random_seed(812)\n\n# Reset states generated by Keras\ntf.keras.backend.clear_session()\n\n# Define the model\nmodel = models.Sequential([\n    # Convolutional layers\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D((2, 2)),\n    layers.Dropout(0.1),\n    \n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D((2, 2)),\n    layers.Dropout(0.1),\n    \n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D((2, 2)),\n    layers.Dropout(0.1),\n    \n    # Flatten and fully connected layers\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(4, activation='softmax')  # 4 classes (0, 1, 2, 3)\n])\n\n# Compile the model\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',  # Use 'categorical_crossentropy' if labels are one-hot encoded\n              metrics=['accuracy'])\n\n# Display model summary\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.544847Z","iopub.status.idle":"2023-10-08T23:47:11.545203Z","shell.execute_reply.started":"2023-10-08T23:47:11.545042Z","shell.execute_reply":"2023-10-08T23:47:11.545058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Training the Model:**\n\n- The model is trained using the training dataset (train_dataset) for 15 epochs, with validation on the validation dataset (val_dataset) after each epoch.","metadata":{}},{"cell_type":"code","source":"history = model.fit(train_dataset, epochs=15, validation_data=val_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.546698Z","iopub.status.idle":"2023-10-08T23:47:11.547021Z","shell.execute_reply.started":"2023-10-08T23:47:11.54686Z","shell.execute_reply":"2023-10-08T23:47:11.546874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Evaluating the Model:**\n\n- The trained model is evaluated on the test dataset (test_dataset), and the loss and accuracy metrics are calculated.","metadata":{}},{"cell_type":"code","source":"# Evaluate the model on the test set\ntest_loss, test_acc = model.evaluate(test_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.548071Z","iopub.status.idle":"2023-10-08T23:47:11.54839Z","shell.execute_reply.started":"2023-10-08T23:47:11.548238Z","shell.execute_reply":"2023-10-08T23:47:11.548253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 60/60 [==============================] - 0s 6ms/step - loss: 0.5457 - accuracy: 0.9516\n- 60/60 [==============================] - 0s 5ms/step - loss: 0.4840 - accuracy: 0.9057\n- 60/60 [==============================] - 0s 5ms/step - loss: 0.4203 - accuracy: 0.9411\n- 60/60 [==============================] - 0s 6ms/step - loss: 0.3777 - accuracy: 0.9203\n- 60/60 [==============================] - 0s 6ms/step - loss: 0.2210 - accuracy: 0.9542\n- 60/60 [==============================] - 0s 3ms/step - loss: 0.1745 - accuracy: 0.9521","metadata":{}},{"cell_type":"code","source":"plot_training_history(history,15)","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.549628Z","iopub.status.idle":"2023-10-08T23:47:11.550029Z","shell.execute_reply.started":"2023-10-08T23:47:11.549822Z","shell.execute_reply":"2023-10-08T23:47:11.549838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoding-Decoding | Outlier detector\n\n- Encoder: The encoder network is responsible for compressing input data, such as images, into a lower-dimensional representation. It learns to capture essential features or patterns in the data. In the context of drowsiness detection, this is crucial because it helps in extracting meaningful characteristics from facial images, like eye and mouth movements.\n\n- Decoder: The decoder network takes the compressed representation (output of the encoder) and tries to reconstruct the original input data. It learns to transform the compressed information back into a format that closely resembles the original input. In this case, the decoder learns to generate facial images based on the compressed features.\n\n- In the context of outlier detection, this architecture is beneficial because during training, the autoencoder learns to reconstruct normal, non-drowsy facial images accurately. Therefore, when presented with unseen or anomalous images (which might indicate drowsiness), the autoencoder will struggle to accurately reconstruct them. The reconstruction error (difference between original and reconstructed images) can then be used as an indicator of anomaly, allowing us to detect potential drowsiness episodes.","metadata":{}},{"cell_type":"code","source":"X_train.shape, X_train[0][5][0]","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.550955Z","iopub.status.idle":"2023-10-08T23:47:11.551256Z","shell.execute_reply.started":"2023-10-08T23:47:11.551108Z","shell.execute_reply":"2023-10-08T23:47:11.551122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Encoder and Decoder Definition:**\n- The code defines two neural networks - an encoder (encoder_net) and a decoder (decoder_net) - for an autoencoder model.","metadata":{}},{"cell_type":"code","source":"encoding_dim = 1024\n\nencoder_net = tf.keras.Sequential(\n  [\n      InputLayer(input_shape=(32, 32, 3)),\n      Conv2D(64, 4, strides=2, padding='same', activation=tf.nn.relu),\n      Conv2D(128, 4, strides=2, padding='same', activation=tf.nn.relu),\n      Conv2D(512, 4, strides=2, padding='same', activation=tf.nn.relu),\n      Flatten(),\n      Dense(encoding_dim,)\n  ])\n\ndecoder_net = tf.keras.Sequential(\n  [\n      InputLayer(input_shape=(encoding_dim,)),\n      Dense(4*4*128),\n      Reshape(target_shape=(4, 4, 128)),\n      Conv2DTranspose(256, 4, strides=2, padding='same', activation=tf.nn.relu),\n      Conv2DTranspose(64, 4, strides=2, padding='same', activation=tf.nn.relu),\n      Conv2DTranspose(3, 4, strides=2, padding='same', activation='sigmoid')\n  ])\n\nprint(encoder_net.summary())\nprint(decoder_net.summary())","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.55233Z","iopub.status.idle":"2023-10-08T23:47:11.552635Z","shell.execute_reply.started":"2023-10-08T23:47:11.552487Z","shell.execute_reply":"2023-10-08T23:47:11.552502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape ,X_train[0][5][0]","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.553731Z","iopub.status.idle":"2023-10-08T23:47:11.554048Z","shell.execute_reply.started":"2023-10-08T23:47:11.553891Z","shell.execute_reply":"2023-10-08T23:47:11.553906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Initializing Outlier Detector:**\n- An outlier detector (od) is created using an instance of OutlierAE class, which utilizes the encoder and decoder defined earlier. The threshold for outlier detection is set at 0.004.\n\n**Training the Outlier Detector:**\n- The outlier detector (od) is trained on the training data (X_train) for 14 epochs.","metadata":{}},{"cell_type":"code","source":"# initialize outlier detector\nod = OutlierAE(threshold=0.004,  # threshold for outlier score\n                encoder_net=encoder_net,  # can also pass AE model instead\n                decoder_net=decoder_net,  # of separate encoder and decoder\n                )\n# train\nod.fit(X_train,\n        epochs=14,\n        verbose=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.555168Z","iopub.status.idle":"2023-10-08T23:47:11.555474Z","shell.execute_reply.started":"2023-10-08T23:47:11.555325Z","shell.execute_reply":"2023-10-08T23:47:11.55534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Reconstruction and Display:**\n- An example image (`X`) from the training data is selected, and its reconstruction (`X_recon`) is obtained using the trained autoencoder. Both the original and reconstructed images are displayed.","metadata":{}},{"cell_type":"code","source":"idx = 8\nX = X_train[idx].reshape(1, 32, 32, 3)\nX_recon = od.ae(X)","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.556471Z","iopub.status.idle":"2023-10-08T23:47:11.556787Z","shell.execute_reply.started":"2023-10-08T23:47:11.556623Z","shell.execute_reply":"2023-10-08T23:47:11.556638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(X.reshape(32, 32, 3))\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.557682Z","iopub.status.idle":"2023-10-08T23:47:11.558301Z","shell.execute_reply.started":"2023-10-08T23:47:11.558138Z","shell.execute_reply":"2023-10-08T23:47:11.558156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(X_recon.numpy().reshape(32, 32, 3))\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.560066Z","iopub.status.idle":"2023-10-08T23:47:11.560408Z","shell.execute_reply.started":"2023-10-08T23:47:11.560229Z","shell.execute_reply":"2023-10-08T23:47:11.560244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_bad_image_predict = od.predict(test_bad_image) #Returns a dictionary of data and metadata","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.561235Z","iopub.status.idle":"2023-10-08T23:47:11.561533Z","shell.execute_reply.started":"2023-10-08T23:47:11.561388Z","shell.execute_reply":"2023-10-08T23:47:11.561403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = X_test[:500]\nprint(X.shape)","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.562838Z","iopub.status.idle":"2023-10-08T23:47:11.563202Z","shell.execute_reply.started":"2023-10-08T23:47:11.563044Z","shell.execute_reply":"2023-10-08T23:47:11.563059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Predicting Outliers on Validation Data:**\n- The outlier detector predicts outliers on a subset of validation data (X) and returns feature and instance scores for each sample.","metadata":{}},{"cell_type":"code","source":"od_preds = od.predict(X,\n                      outlier_type='instance',    # use 'feature' or 'instance' level\n                      return_feature_score=True,  # scores used to determine outliers\n                      return_instance_score=True)\nprint(list(od_preds['data'].keys()))","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.564593Z","iopub.status.idle":"2023-10-08T23:47:11.565185Z","shell.execute_reply.started":"2023-10-08T23:47:11.565019Z","shell.execute_reply":"2023-10-08T23:47:11.565037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Displaying Instance Scores:**\n- The instance scores are plotted, showing which samples are detected as outliers.","metadata":{}},{"cell_type":"code","source":"od_preds['data']['is_outlier']","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.566371Z","iopub.status.idle":"2023-10-08T23:47:11.566697Z","shell.execute_reply.started":"2023-10-08T23:47:11.566529Z","shell.execute_reply":"2023-10-08T23:47:11.566543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Threshold Inspection:**\n- The current threshold value for outlier detection is printed.","metadata":{}},{"cell_type":"code","source":"target = np.zeros(X.shape[0],).astype(int)  # all normal CIFAR10 training instances\nlabels = ['normal', 'outlier']\nplot_instance_score(od_preds, target, labels, od.threshold)","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.567964Z","iopub.status.idle":"2023-10-08T23:47:11.56828Z","shell.execute_reply.started":"2023-10-08T23:47:11.568131Z","shell.execute_reply":"2023-10-08T23:47:11.568145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Threshold Adjustment:**\n- The threshold is adjusted based on percentiles of instance scores, potentially enhancing the detector's sensitivity to outliers.","metadata":{}},{"cell_type":"code","source":"#Check the threshold value. Should be the same as defined before. \nprint(\"Current threshold value is: \", od.threshold)","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.569348Z","iopub.status.idle":"2023-10-08T23:47:11.569707Z","shell.execute_reply.started":"2023-10-08T23:47:11.569505Z","shell.execute_reply":"2023-10-08T23:47:11.56952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"od.infer_threshold(X_train, outlier_type='instance', threshold_perc=99.0)\n# od.threshold = 0.005\nprint(\"Current threshold value is: \", od.threshold)","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.570636Z","iopub.status.idle":"2023-10-08T23:47:11.571364Z","shell.execute_reply.started":"2023-10-08T23:47:11.571177Z","shell.execute_reply":"2023-10-08T23:47:11.571196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Inspecting Specific Outliers:**\n- Specific instances (indexed as `[5, 70, 95, 190]`) are inspected for outlier predictions.","metadata":{}},{"cell_type":"code","source":"od_preds['data']['is_outlier'][[5, 70, 95, 190]]","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.572228Z","iopub.status.idle":"2023-10-08T23:47:11.572538Z","shell.execute_reply.started":"2023-10-08T23:47:11.57238Z","shell.execute_reply":"2023-10-08T23:47:11.572395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Reconstructing Images:**\n- Images are reconstructed using the autoencoder (od.ae(X)).\n\n**Feature Outlier Image Plotting:**\n- Feature outlier images are plotted for specific instances, displaying the original, reconstructed, and outlier versions.","metadata":{}},{"cell_type":"code","source":"X_recon = od.ae(X).numpy()\nplot_feature_outlier_image(od_preds, \n                           X, \n                           X_recon=X_recon,\n                           instance_ids=[5, 70, 95, 190],  # pass a list with indices of instances to display\n                           max_instances=5,  # max nb of instances to display\n                           outliers_only=False)  # only show outlier predictions","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.57348Z","iopub.status.idle":"2023-10-08T23:47:11.573835Z","shell.execute_reply.started":"2023-10-08T23:47:11.573661Z","shell.execute_reply":"2023-10-08T23:47:11.573684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load unseen images\n\nA function `load_single_image_from_link` is defined to load images from file paths and resize them if necessary.","metadata":{}},{"cell_type":"code","source":"import requests\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef load_single_image_from_link(image_link, target_shape=(32,32)):\n#     response = requests.get(image_link)\n    image = Image.open(image_link)\n    image = image.convert(\"RGB\")  # Convert to RGB mode\n    if target_shape is not None:\n        image = image.resize(target_shape)  # Resize image\n    return np.array(image)\n\n# Load a single image from a link\nimage_link = '/kaggle/input/face-pose-dataset/down/aug_0_112.jpg'\nimage_link1 = '/kaggle/input/face-pose-dataset/right/Bill_Frist_0005.jpg'\nimage_link2 = '/kaggle/input/face-pose-dataset/left/Anders_Ebbeson_0001.jpg'\nimage_link3 = '/kaggle/input/yawn-eye-dataset-new/dataset_new/test/Open/_107.jpg'\n\nimage_array = load_single_image_from_link(image_link)\nimage_array1 = load_single_image_from_link(image_link1)\nimage_array2 = load_single_image_from_link(image_link2)\nimage_array3 = load_single_image_from_link(image_link3)\n\n# Display the image\nplt.figure(figsize=(3,3))\nplt.imshow(image_array)\nplt.show()\n# Display the image\nplt.figure(figsize=(3,3))\nplt.imshow(image_array1)\nplt.show()\n# Display the image\nplt.figure(figsize=(3,3))\nplt.imshow(image_array2)\nplt.show()\n# Display the image\nplt.figure(figsize=(3,3))\nplt.imshow(image_array3)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.574925Z","iopub.status.idle":"2023-10-08T23:47:11.575501Z","shell.execute_reply.started":"2023-10-08T23:47:11.575336Z","shell.execute_reply":"2023-10-08T23:47:11.575353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Preparing Unseen Images for Prediction:**\n- The unseen images are normalized and prepared for input to the outlier detector.","metadata":{}},{"cell_type":"code","source":"unseen_image = np.array([image_array,image_array1,image_array2,image_array3])\nunseen_image = unseen_image/ 255.0\nunseen_image[0][0][0]","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.576755Z","iopub.status.idle":"2023-10-08T23:47:11.577137Z","shell.execute_reply.started":"2023-10-08T23:47:11.576938Z","shell.execute_reply":"2023-10-08T23:47:11.576954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Predicting Outliers in Unseen Images:**\n- The outlier detector predicts whether the unseen images are outliers. The results are printed.","metadata":{}},{"cell_type":"code","source":"test_bad_image_predict = od.predict(unseen_image) #Returns a dictionary of data and metadata\nprint('Is outlier:',test_bad_image_predict['data']['is_outlier'])\n# - First 2 images are detected as outliers","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.578606Z","iopub.status.idle":"2023-10-08T23:47:11.578976Z","shell.execute_reply.started":"2023-10-08T23:47:11.578793Z","shell.execute_reply":"2023-10-08T23:47:11.578809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Displaying Outlier Predictions:**\n- Feature outlier images are plotted for the unseen images, showing the original, reconstructed, and outlier versions.","metadata":{}},{"cell_type":"code","source":"od_preds = od.predict(unseen_image,\n                      outlier_type='instance',    # use 'feature' or 'instance' level\n                      return_feature_score=True,  # scores used to determine outliers\n                      return_instance_score=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.580014Z","iopub.status.idle":"2023-10-08T23:47:11.580315Z","shell.execute_reply.started":"2023-10-08T23:47:11.580168Z","shell.execute_reply":"2023-10-08T23:47:11.580183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_recon = od.ae(unseen_image).numpy()\nplot_feature_outlier_image(od_preds, \n                           unseen_image, \n                           X_recon=X_recon,\n#                            instance_ids=[5, 70, 95, 190],  # pass a list with indices of instances to display\n#                            max_instances=3,  # max nb of instances to display\n                           outliers_only=False)  # only show outlier predictions","metadata":{"execution":{"iopub.status.busy":"2023-10-08T23:47:11.581539Z","iopub.status.idle":"2023-10-08T23:47:11.581903Z","shell.execute_reply.started":"2023-10-08T23:47:11.581721Z","shell.execute_reply":"2023-10-08T23:47:11.581737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}